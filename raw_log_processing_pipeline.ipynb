{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "log_pattern = re.compile(r'(\\d{6}) (\\d{6}) (\\d+) (\\w+) ([\\w.$]+): (.+)')\n",
    "block_id_pattern = re.compile(r'blk_[\\d-]+')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_log_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        raw_logs = file.readlines()\n",
    "    return [log.strip() for log in raw_logs]\n",
    "\n",
    "def parse_log(log):\n",
    "    match = log_pattern.match(log)\n",
    "    if match:\n",
    "        return match.groups()\n",
    "    return None\n",
    "\n",
    "def extract_block_id(log):\n",
    "    match = block_id_pattern.search(log)\n",
    "    if match:\n",
    "        return match.group()\n",
    "    return None\n",
    "\n",
    "def parse_logs(logs):\n",
    "    parsed_logs = [parse_log(log) for log in logs]\n",
    "    parsed_logs = [log for log in parsed_logs if log is not None]\n",
    "    return parsed_logs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>process_id</th>\n",
       "      <th>log_level</th>\n",
       "      <th>component</th>\n",
       "      <th>message</th>\n",
       "      <th>block_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>081109</td>\n",
       "      <td>203518</td>\n",
       "      <td>143</td>\n",
       "      <td>INFO</td>\n",
       "      <td>dfs.DataNode$DataXceiver</td>\n",
       "      <td>Receiving block blk_-1608999687919862906 src: ...</td>\n",
       "      <td>blk_-1608999687919862906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>081109</td>\n",
       "      <td>203518</td>\n",
       "      <td>35</td>\n",
       "      <td>INFO</td>\n",
       "      <td>dfs.FSNamesystem</td>\n",
       "      <td>BLOCK* NameSystem.allocateBlock: /mnt/hadoop/m...</td>\n",
       "      <td>blk_-1608999687919862906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>081109</td>\n",
       "      <td>203519</td>\n",
       "      <td>143</td>\n",
       "      <td>INFO</td>\n",
       "      <td>dfs.DataNode$DataXceiver</td>\n",
       "      <td>Receiving block blk_-1608999687919862906 src: ...</td>\n",
       "      <td>blk_-1608999687919862906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>081109</td>\n",
       "      <td>203519</td>\n",
       "      <td>145</td>\n",
       "      <td>INFO</td>\n",
       "      <td>dfs.DataNode$DataXceiver</td>\n",
       "      <td>Receiving block blk_-1608999687919862906 src: ...</td>\n",
       "      <td>blk_-1608999687919862906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>081109</td>\n",
       "      <td>203519</td>\n",
       "      <td>145</td>\n",
       "      <td>INFO</td>\n",
       "      <td>dfs.DataNode$PacketResponder</td>\n",
       "      <td>PacketResponder 1 for block blk_-1608999687919...</td>\n",
       "      <td>blk_-1608999687919862906</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     date    time process_id log_level                     component  \\\n",
       "0  081109  203518        143      INFO      dfs.DataNode$DataXceiver   \n",
       "1  081109  203518         35      INFO              dfs.FSNamesystem   \n",
       "2  081109  203519        143      INFO      dfs.DataNode$DataXceiver   \n",
       "3  081109  203519        145      INFO      dfs.DataNode$DataXceiver   \n",
       "4  081109  203519        145      INFO  dfs.DataNode$PacketResponder   \n",
       "\n",
       "                                             message                  block_id  \n",
       "0  Receiving block blk_-1608999687919862906 src: ...  blk_-1608999687919862906  \n",
       "1  BLOCK* NameSystem.allocateBlock: /mnt/hadoop/m...  blk_-1608999687919862906  \n",
       "2  Receiving block blk_-1608999687919862906 src: ...  blk_-1608999687919862906  \n",
       "3  Receiving block blk_-1608999687919862906 src: ...  blk_-1608999687919862906  \n",
       "4  PacketResponder 1 for block blk_-1608999687919...  blk_-1608999687919862906  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read raw log file\n",
    "raw_logs = load_log_file('HDFS.log')\n",
    "\n",
    "# Parse raw logs\n",
    "parsed_logs = parse_logs(raw_logs)\n",
    "\n",
    "# Extract and add block id to parsed logs from log message\n",
    "block_ids = [extract_block_id(log[5]) for log in parsed_logs]\n",
    "parsed_logs = [log + (block_id,) for log, block_id in zip(parsed_logs, block_ids)]\n",
    "\n",
    "# Create DataFrame from parsed logs\n",
    "columns = ['date', 'time', 'process_id', 'log_level', 'component', 'message', 'block_id']\n",
    "df = pd.DataFrame(parsed_logs, columns=columns)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BlockId</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>blk_-1608999687919862906</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>blk_7503483334202473044</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>blk_-3544583377289625738</td>\n",
       "      <td>Anomaly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>blk_-9073992586687739851</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>blk_7854771516489510256</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    BlockId    Label\n",
       "0  blk_-1608999687919862906   Normal\n",
       "1   blk_7503483334202473044   Normal\n",
       "2  blk_-3544583377289625738  Anomaly\n",
       "3  blk_-9073992586687739851   Normal\n",
       "4   blk_7854771516489510256   Normal"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read ground truth file\n",
    "ground_truth = pd.read_csv('anomaly_label.csv')\n",
    "ground_truth.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>process_id</th>\n",
       "      <th>log_level</th>\n",
       "      <th>component</th>\n",
       "      <th>message</th>\n",
       "      <th>block_id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>081109</td>\n",
       "      <td>203518</td>\n",
       "      <td>143</td>\n",
       "      <td>INFO</td>\n",
       "      <td>dfs.DataNode$DataXceiver</td>\n",
       "      <td>Receiving block blk_-1608999687919862906 src: ...</td>\n",
       "      <td>blk_-1608999687919862906</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>081109</td>\n",
       "      <td>203518</td>\n",
       "      <td>35</td>\n",
       "      <td>INFO</td>\n",
       "      <td>dfs.FSNamesystem</td>\n",
       "      <td>BLOCK* NameSystem.allocateBlock: /mnt/hadoop/m...</td>\n",
       "      <td>blk_-1608999687919862906</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>081109</td>\n",
       "      <td>203519</td>\n",
       "      <td>143</td>\n",
       "      <td>INFO</td>\n",
       "      <td>dfs.DataNode$DataXceiver</td>\n",
       "      <td>Receiving block blk_-1608999687919862906 src: ...</td>\n",
       "      <td>blk_-1608999687919862906</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>081109</td>\n",
       "      <td>203519</td>\n",
       "      <td>145</td>\n",
       "      <td>INFO</td>\n",
       "      <td>dfs.DataNode$DataXceiver</td>\n",
       "      <td>Receiving block blk_-1608999687919862906 src: ...</td>\n",
       "      <td>blk_-1608999687919862906</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>081109</td>\n",
       "      <td>203519</td>\n",
       "      <td>145</td>\n",
       "      <td>INFO</td>\n",
       "      <td>dfs.DataNode$PacketResponder</td>\n",
       "      <td>PacketResponder 1 for block blk_-1608999687919...</td>\n",
       "      <td>blk_-1608999687919862906</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     date    time process_id log_level                     component  \\\n",
       "0  081109  203518        143      INFO      dfs.DataNode$DataXceiver   \n",
       "1  081109  203518         35      INFO              dfs.FSNamesystem   \n",
       "2  081109  203519        143      INFO      dfs.DataNode$DataXceiver   \n",
       "3  081109  203519        145      INFO      dfs.DataNode$DataXceiver   \n",
       "4  081109  203519        145      INFO  dfs.DataNode$PacketResponder   \n",
       "\n",
       "                                             message  \\\n",
       "0  Receiving block blk_-1608999687919862906 src: ...   \n",
       "1  BLOCK* NameSystem.allocateBlock: /mnt/hadoop/m...   \n",
       "2  Receiving block blk_-1608999687919862906 src: ...   \n",
       "3  Receiving block blk_-1608999687919862906 src: ...   \n",
       "4  PacketResponder 1 for block blk_-1608999687919...   \n",
       "\n",
       "                   block_id  target  \n",
       "0  blk_-1608999687919862906  Normal  \n",
       "1  blk_-1608999687919862906  Normal  \n",
       "2  blk_-1608999687919862906  Normal  \n",
       "3  blk_-1608999687919862906  Normal  \n",
       "4  blk_-1608999687919862906  Normal  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Map block id to anomaly label in ground truth\n",
    "block_id_to_label = dict(zip(ground_truth['BlockId'], ground_truth['Label']))\n",
    "df['target'] = df['block_id'].map(block_id_to_label)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "Normal     10887379\n",
       "Anomaly      288250\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/dhairyaparikh/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/dhairyaparikh/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/dhairyaparikh/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/dhairyaparikh/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "# Ensure NLTK resources are downloaded\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Step 1: Remove the substring \"blk_<block-id>\"\n",
    "def remove_block_id(text):\n",
    "    return re.sub(r'blk_\\w+', '', text)\n",
    "\n",
    "# Step 2: Convert text to lowercase\n",
    "def to_lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "# Step 3: Tokenize the text\n",
    "def tokenize(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "# Step 4: Remove punctuation\n",
    "def remove_punctuation(tokens):\n",
    "    return [word for word in tokens if word.isalnum()]\n",
    "\n",
    "# Step 5: Remove stop words\n",
    "def remove_stopwords(tokens):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    return [word for word in tokens if word not in stop_words]\n",
    "\n",
    "# Step 6: Perform stemming\n",
    "def stem_words(tokens):\n",
    "    stemmer = PorterStemmer()\n",
    "    return [stemmer.stem(word) for word in tokens]\n",
    "\n",
    "# Step 7: Perform lemmatization\n",
    "def lemmatize_words(tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "# Complete preprocessing pipeline\n",
    "def preprocess_log(log):\n",
    "    # Remove the substring \"blk_<block-id>\"\n",
    "    log = remove_block_id(log)   \n",
    "    # Convert text to lowercase\n",
    "    log = to_lowercase(log) \n",
    "    # Tokenize the text\n",
    "    tokens = tokenize(log) \n",
    "    # Remove punctuation\n",
    "    tokens = remove_punctuation(tokens)\n",
    "    # Remove stop words\n",
    "    tokens = remove_stopwords(tokens)\n",
    "    # Perform stemming\n",
    "    tokens = stem_words(tokens)   \n",
    "    # Perform lemmatization\n",
    "    tokens = lemmatize_words(tokens)\n",
    "\n",
    "    # convert list of tokens to a string\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing to log messages\n",
    "df['message_processed'] = df['message'].apply(preprocess_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0           [receiv, block, src, dest]\n",
       "1                              [block]\n",
       "2           [receiv, block, src, dest]\n",
       "3           [receiv, block, src, dest]\n",
       "4    [packetrespond, 1, block, termin]\n",
       "Name: message_processed, dtype: object"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['message_processed'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9762630831550436\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Anomaly       0.77      0.11      0.20     57650\n",
      "      Normal       0.98      1.00      0.99   2177476\n",
      "\n",
      "    accuracy                           0.98   2235126\n",
      "   macro avg       0.87      0.56      0.59   2235126\n",
      "weighted avg       0.97      0.98      0.97   2235126\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and testing sets using stratified sampling\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df['message_processed']\n",
    "y = df['target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Convert text data to numerical data using TF-IDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train.map(lambda x: ' '.join(x)))\n",
    "X_test_tfidf = vectorizer.transform(X_test.map(lambda x: ' '.join(x)))\n",
    "\n",
    "# Train a Logistic Regression model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "from sklearn.metrics import classification_report\n",
    "# Print accuracy\n",
    "\n",
    "print('Accuracy:', model.score(X_test_tfidf, y_test))\n",
    "\n",
    "y_pred = model.predict(X_test_tfidf)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
